import warnings

warnings.filterwarnings("ignore", message=".*pkg_resources.*deprecated.*")

import hashlib
import json
import logging
import os
import re
import time
from functools import lru_cache
from http import HTTPStatus
from typing import Dict, List, Tuple, Optional

import dashscope
import faiss
import jieba
import numpy as np
import pandas as pd
from openai import OpenAI
from rank_bm25 import BM25Okapi
from sqlalchemy.inspection import inspect
from sqlalchemy.sql.expression import text

from agent.text2sql.state.agent_state import AgentState, ExecutionResult
from model.db_connection_pool import get_db_pool

# æ—¥å¿—é…ç½®
logger = logging.getLogger(__name__)

# æ•°æ®åº“è¿æ¥æ± 
db_pool = get_db_pool()

# å‘é‡æ£€ç´¢é…ç½®
USE_DASHSCOPE_EMBEDDING = True  # ä½¿ç”¨é˜¿é‡Œäº‘åµŒå…¥æ¨¡å‹
FORCE_REBUILD_VECTOR_INDEX = os.getenv("FORCE_REBUILD_VECTOR_INDEX", "false").lower() == "true"

# å‘é‡ç´¢å¼•å­˜å‚¨è·¯å¾„
VECTOR_INDEX_DIR = "./vector_index"
os.makedirs(VECTOR_INDEX_DIR, exist_ok=True)

INDEX_FILE = os.path.join(VECTOR_INDEX_DIR, "schema.index")
METADATA_FILE = os.path.join(VECTOR_INDEX_DIR, "metadata.json")

# é‡æ’æ¨¡å‹
RERANK_MODEL_NAME = os.getenv("RERANK_MODEL_NAME")
#  åµŒå…¥æ¨¡å‹
EMBEDDING_MODEL_NAME = os.getenv("EMBEDDING_MODEL_NAME")

# åˆå§‹åŒ– DashScope å®¢æˆ·ç«¯
if USE_DASHSCOPE_EMBEDDING:
    MODEL_API_KEY = os.getenv("SMALL_MODEL_API_KEY")
    MODEL_BASE_URL = os.getenv("SMALL_MODEL_BASE_URL")
    if not MODEL_API_KEY:
        raise ValueError("ç¯å¢ƒå˜é‡ MODEL_API_KEY æœªè®¾ç½®ï¼Œæ— æ³•åˆå§‹åŒ–åµŒå…¥æ¨¡å‹å®¢æˆ·ç«¯")
    client = OpenAI(api_key=MODEL_API_KEY, base_url=MODEL_BASE_URL)


class DatabaseService:
    """
    æ”¯æŒæ··åˆæ£€ç´¢ï¼ˆBM25 + å‘é‡ï¼‰ä¸ç´¢å¼•æŒä¹…åŒ–çš„æ•°æ®åº“æœåŠ¡ã€‚
    æä¾›è¡¨ç»“æ„æ£€ç´¢ã€SQL æ‰§è¡Œã€é”™è¯¯ä¿®æ­£ SQL æ‰§è¡Œç­‰åŠŸèƒ½ã€‚

    æ ¸å¿ƒæ‰§è¡Œæµç¨‹ï¼š
    1. åˆå§‹åŒ–é˜¶æ®µï¼šå»ºç«‹æ•°æ®åº“è¿æ¥æ± ï¼Œé…ç½®æ£€ç´¢å‚æ•°
    2. è¡¨ç»“æ„æ£€ç´¢ï¼šé€šè¿‡æ··åˆæ£€ç´¢è·å–ä¸ç”¨æˆ·æŸ¥è¯¢æœ€ç›¸å…³çš„è¡¨ç»“æ„
       - è·å–æ‰€æœ‰è¡¨ç»“æ„ä¿¡æ¯
       - æ„å»ºæ–‡æ¡£æ–‡æœ¬ç”¨äºæ£€ç´¢
       - åˆå§‹åŒ–å‘é‡ç´¢å¼•ï¼ˆåŠ è½½æˆ–é‡å»ºï¼‰
       - BM25 å…³é”®è¯åŒ¹é…æ£€ç´¢
       - å‘é‡ç›¸ä¼¼åº¦æ£€ç´¢
       - RRF èåˆä¸¤ç§æ£€ç´¢ç»“æœ
       - ä½¿ç”¨ DashScope é‡æ’åºå™¨ä¼˜åŒ–æ’åº
    3. SQL æ‰§è¡Œï¼šæ‰§è¡ŒæŸ¥è¯¢è¯­å¥å¹¶è¿”å›ç»“æœ
    """

    def __init__(self):
        self._engine = db_pool.get_engine()
        self._faiss_index: Optional[faiss.Index] = None
        self._table_names: List[str] = []
        self._corpus: List[str] = []
        self._tokenized_corpus: List[List[str]] = []
        self._index_initialized: bool = False
        self.USE_RERANKER: bool = True  # æ˜¯å¦å¯ç”¨é‡æ’åºå™¨

    @staticmethod
    def _tokenize_text(text_str: str) -> List[str]:
        """
        å¯¹ä¸­æ–‡/è‹±æ–‡æ–‡æœ¬è¿›è¡Œåˆ†è¯ï¼Œè¿‡æ»¤æ ‡ç‚¹ç¬¦å·ã€‚

        Args:
            text_str (str): è¾“å…¥æ–‡æœ¬

        Returns:
            List[str]: åˆ†è¯åçš„ token åˆ—è¡¨
        """
        filtered_text = re.sub(r"[^\u4e00-\u9fa5a-zA-Z0-9]", " ", text_str)
        tokens = jieba.lcut(filtered_text, cut_all=False)
        return [token.strip() for token in tokens if token.strip()]

    def _get_table_comment(self, table_name: str) -> str:
        """
        ä» information_schema ä¸­è·å–æŒ‡å®šè¡¨çš„æ³¨é‡Šã€‚

        Args:
            table_name (str): è¡¨å

        Returns:
            str: è¡¨æ³¨é‡Šï¼Œè‹¥æ— åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²
        """
        try:
            query = text(
                """
                SELECT table_comment
                FROM information_schema.tables
                WHERE table_schema = DATABASE()
                  AND table_name = :table_name;
                """
            )
            with self._engine.connect() as conn:
                result = conn.execute(query, {"table_name": table_name})
                row = result.fetchone()
                return (row[0] or "").strip()
        except Exception as e:
            logger.warning(f"âš ï¸ è·å–è¡¨ {table_name} æ³¨é‡Šå¤±è´¥: {e}")
            return ""

    @staticmethod
    def _build_document(table_name: str, table_info: dict) -> str:
        """
        æ„å»ºç”¨äºæ£€ç´¢çš„æ–‡æ¡£æ–‡æœ¬ï¼ˆè¡¨å + æ³¨é‡Š + å­—æ®µå + å­—æ®µæ³¨é‡Šï¼‰ã€‚

        Args:
            table_name (str): è¡¨å
            table_info (dict): åŒ…å«åˆ—ã€å¤–é”®ã€æ³¨é‡Šç­‰ä¿¡æ¯çš„å­—å…¸

        Returns:
            str: æ‹¼æ¥åçš„æ–‡æ¡£æ–‡æœ¬
        """
        parts = [table_name]
        if table_info.get("table_comment"):
            parts.append(table_info["table_comment"])
        for col_name, col_info in table_info.get("columns", {}).items():
            parts.append(col_name)
            if col_info.get("comment"):
                parts.append(col_info["comment"])
        return " ".join(parts)

    @lru_cache(maxsize=1)
    def _fetch_all_table_info(self) -> Dict[str, Dict]:
        """
        è·å–æ•°æ®åº“ä¸­æ‰€æœ‰è¡¨çš„ç»“æ„ä¿¡æ¯ï¼ˆå¸¦ LRU ç¼“å­˜ï¼‰ã€‚

        Returns:
            Dict[str, Dict]: è¡¨å â†’ è¡¨ç»“æ„ä¿¡æ¯çš„å­—å…¸
        """
        start_time = time.time()
        inspector = inspect(self._engine)
        table_names = inspector.get_table_names()
        logger.info(f"ğŸ” å¼€å§‹åŠ è½½ {len(table_names)} å¼ è¡¨çš„ schema ä¿¡æ¯...")

        table_info = {}
        for table_name in table_names:
            try:
                columns = {}
                for col in inspector.get_columns(table_name):
                    columns[col["name"]] = {
                        "type": str(col["type"]),
                        "comment": str(col["comment"] or ""),
                    }

                foreign_keys = [
                    f"{fk['constrained_columns'][0]} -> {fk['referred_table']}.{fk['referred_columns'][0]}"
                    for fk in inspector.get_foreign_keys(table_name)
                ]

                table_comment = self._get_table_comment(table_name)

                table_info[table_name] = {
                    "columns": columns,
                    "foreign_keys": foreign_keys,
                    "table_comment": table_comment,
                }
            except Exception as e:
                logger.error(f"âŒ è¯»å–è¡¨ {table_name} ç»“æ„å¤±è´¥: {e}")

        elapsed = time.time() - start_time
        logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(table_info)} å¼ è¡¨ï¼Œè€—æ—¶ {elapsed:.2f}s")
        return table_info

    @staticmethod
    def _generate_schema_fingerprint(table_info: Dict[str, Dict]) -> str:
        """
        ç”Ÿæˆ schema çš„æŒ‡çº¹ï¼ˆMD5 å“ˆå¸Œï¼‰ï¼Œç”¨äºæ£€æµ‹å˜æ›´ã€‚

        Args:
            table_info (Dict[str, Dict]): è¡¨ç»“æ„ä¿¡æ¯

        Returns:
            str: MD5 æŒ‡çº¹
        """
        fingerprint_data = {}
        for table_name, info in table_info.items():
            fingerprint_data[table_name] = {
                "comment": info.get("table_comment", ""),
                "columns": sorted(
                    [f"{col_name}:{col_info.get('comment', '')}" for col_name, col_info in info["columns"].items()]
                ),
            }
        json_str = json.dumps(fingerprint_data, sort_keys=True, ensure_ascii=False)
        return hashlib.md5(json_str.encode("utf-8")).hexdigest()

    def _load_vector_index(self, table_info: Dict[str, Dict]) -> bool:
        """
        ä»ç£ç›˜åŠ è½½ FAISS å‘é‡ç´¢å¼•å’Œå…ƒæ•°æ®ã€‚

        Args:
            table_info (Dict[str, Dict]): å½“å‰è¡¨ç»“æ„

        Returns:
            bool: æ˜¯å¦åŠ è½½æˆåŠŸ
        """
        if not (os.path.exists(INDEX_FILE) and os.path.exists(METADATA_FILE)):
            logger.info("âŒ å‘é‡ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†é‡å»º")
            return False

        try:
            with open(METADATA_FILE, "r", encoding="utf-8") as f:
                metadata = json.load(f)

            current_fingerprint = self._generate_schema_fingerprint(table_info)
            if metadata.get("fingerprint") != current_fingerprint:
                logger.info("ğŸ”„ æ•°æ®åº“ schema å·²å˜æ›´ï¼Œéœ€é‡å»ºå‘é‡ç´¢å¼•")
                return False

            self._faiss_index = faiss.read_index(INDEX_FILE)
            self._table_names = metadata["table_names"]
            self._corpus = metadata["corpus"]

            logger.info(f"ğŸ‰ æˆåŠŸåŠ è½½å‘é‡ç´¢å¼•ï¼ŒåŒ…å« {len(self._table_names)} å¼ è¡¨")
            return True

        except Exception as e:
            logger.warning(f"âš ï¸ åŠ è½½å‘é‡ç´¢å¼•å¤±è´¥: {e}ï¼Œå°†é‡å»º")
            return False

    def _save_vector_index(self, table_info: Dict[str, Dict]):
        """
        å°† FAISS ç´¢å¼•å’Œå…ƒæ•°æ®ä¿å­˜åˆ°ç£ç›˜ã€‚

        Args:
            table_info (Dict[str, Dict]): å½“å‰è¡¨ç»“æ„
        """
        if self._faiss_index is None:
            return

        faiss.write_index(self._faiss_index, INDEX_FILE)

        metadata = {
            "table_names": self._table_names,
            "corpus": self._corpus,
            "fingerprint": self._generate_schema_fingerprint(table_info),
            "updated_at": pd.Timestamp.now().isoformat(),
        }
        with open(METADATA_FILE, "w", encoding="utf-8") as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)

        logger.info(f"âœ… å‘é‡ç´¢å¼•å·²ä¿å­˜è‡³: {INDEX_FILE}")

    @staticmethod
    def _create_embeddings_with_dashscope(texts: List[str]) -> np.ndarray:
        """
        ä½¿ç”¨ DashScope API ç”Ÿæˆæ–‡æœ¬åµŒå…¥å‘é‡ã€‚

        Args:
            texts (List[str]): è¾“å…¥æ–‡æœ¬åˆ—è¡¨

        Returns:
            np.ndarray: åµŒå…¥å‘é‡æ•°ç»„
        """
        logger.info("ğŸŒ è°ƒç”¨ DashScope æ–‡æœ¬åµŒå…¥ API...")
        start_time = time.time()
        embeddings = []
        for doc in texts:
            try:
                response = client.embeddings.create(model=EMBEDDING_MODEL_NAME, input=doc)
                embeddings.append(response.data[0].embedding)
            except Exception as e:
                logger.error(f"âŒ åµŒå…¥ç”Ÿæˆå¤±è´¥ ({doc[:30]}...): {e}")
                embeddings.append(np.zeros(1024))  # å ä½ç¬¦

        embeddings = np.array(embeddings).astype("float32")
        faiss.normalize_L2(embeddings)
        logger.info(f"âœ… åµŒå…¥ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶ {time.time() - start_time:.2f}s")
        return embeddings

    def _initialize_vector_index(self, table_info: Dict[str, Dict]):
        """
        åˆå§‹åŒ– FAISS å‘é‡ç´¢å¼•ï¼šåŠ è½½æˆ–é‡å»ºã€‚

        Args:
            table_info (Dict[str, Dict]): è¡¨ç»“æ„ä¿¡æ¯
        """
        if self._index_initialized:
            return

        if FORCE_REBUILD_VECTOR_INDEX:
            logger.info("ğŸ’¡ å¼ºåˆ¶é‡å»ºå‘é‡ç´¢å¼•ï¼ˆç¯å¢ƒå˜é‡è§¦å‘ï¼‰")
        elif self._load_vector_index(table_info):
            self._index_initialized = True
            return

        # æ„å»ºæ–°ç´¢å¼•
        logger.info("ğŸ—ï¸ å¼€å§‹æ„å»ºæ–°çš„å‘é‡ç´¢å¼•...")
        start_time = time.time()

        self._table_names = list(table_info.keys())
        self._corpus = [self._build_document(name, info) for name, info in table_info.items()]

        # ç”ŸæˆåµŒå…¥
        embeddings = self._create_embeddings_with_dashscope(self._corpus)

        # åˆå§‹åŒ– FAISS ç´¢å¼•
        dimension = embeddings.shape[1]
        self._faiss_index = faiss.IndexFlatIP(dimension)  # å†…ç§¯ = ä½™å¼¦ç›¸ä¼¼åº¦
        self._faiss_index.add(embeddings)

        # ä¿å­˜ç´¢å¼•
        self._save_vector_index(table_info)

        elapsed = time.time() - start_time
        logger.info(f"ğŸ‰ å‘é‡ç´¢å¼•æ„å»ºå®Œæˆï¼Œå…± {len(self._table_names)} å¼ è¡¨ï¼Œè€—æ—¶ {elapsed:.2f}s")
        self._index_initialized = True

    def _retrieve_by_vector(self, query: str, top_k: int = 10) -> List[int]:
        """
        ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦æ£€ç´¢æœ€ç›¸å…³çš„è¡¨ã€‚

        è¯¥æ–¹æ³•é€šè¿‡è°ƒç”¨ DashScope çš„æ–‡æœ¬åµŒå…¥æ¨¡å‹ï¼Œå°†ç”¨æˆ·æŸ¥è¯¢è½¬æ¢ä¸ºå‘é‡ï¼Œ
        å¹¶ä½¿ç”¨ FAISS å‘é‡ç´¢å¼•æœç´¢ä¸ä¹‹æœ€ç›¸ä¼¼çš„ top_k ä¸ªè¡¨ç»“æ„æ–‡æ¡£ã€‚

        Faissï¼ˆFacebook AI Similarity Searchï¼‰æ˜¯ç”± Meta (Facebook) å¼€å‘çš„ä¸€ä¸ªé«˜æ•ˆçš„ç›¸ä¼¼æ€§æœç´¢åº“
        faiss-gpu/faiss-cpu ä¸¤ä¸ªç‰ˆæœ¬

        æ•°æ®æ˜ å°„å…³ç³»è¯´æ˜ï¼š
        - æ„å»ºæ˜ å°„ï¼šåœ¨ç´¢å¼•åˆå§‹åŒ–æ—¶ï¼ŒæŒ‰ç…§ç›¸åŒé¡ºåºç»´æŠ¤ _table_namesï¼ˆè¡¨ååˆ—è¡¨ï¼‰å’Œ _corpusï¼ˆæ–‡æ¡£åˆ—è¡¨ï¼‰
        - å‘é‡æ£€ç´¢ï¼šFAISS è¿”å›ç›¸ä¼¼æ–‡æ¡£åœ¨ _corpus ä¸­çš„ç´¢å¼•ä½ç½®
        - ç´¢å¼•è½¬æ¢ï¼šé€šè¿‡ _table_names[index] å°†ç´¢å¼•ä½ç½®è½¬æ¢ä¸ºå…·ä½“çš„è¡¨å
        - ç»“æœä½¿ç”¨ï¼šä½¿ç”¨è¿™äº›è¡¨åä»åŸå§‹è¡¨ä¿¡æ¯ä¸­æå–è¯¦ç»†ç»“æ„

        Args:
            query (str): ç”¨æˆ·è¾“å…¥çš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢ï¼Œä¾‹å¦‚ï¼š"æŸ¥æ‰¾æœ€è¿‘ä¸€å‘¨çš„è®¢å•ä¿¡æ¯"
            top_k (int): éœ€è¦è¿”å›çš„æœ€ç›¸ä¼¼è¡¨çš„æ•°é‡ï¼Œé»˜è®¤å€¼ä¸º 10

        Returns:
            List[int]: ä¸ç”¨æˆ·æŸ¥è¯¢æœ€ç›¸ä¼¼çš„è¡¨åœ¨ corpus ä¸­çš„ç´¢å¼•åˆ—è¡¨ï¼Œ
                       æŒ‰ç…§ç›¸ä¼¼åº¦ä»é«˜åˆ°ä½æ’åº
        """
        try:
            response = client.embeddings.create(model=EMBEDDING_MODEL_NAME, input=query)
            query_vec = np.array([response.data[0].embedding]).astype("float32")
            faiss.normalize_L2(query_vec)
            _, indices = self._faiss_index.search(query_vec, top_k)
            return indices[0].tolist()
        except Exception as e:
            logger.error(f"âŒ å‘é‡æ£€ç´¢å¤±è´¥: {e}")
            return []

    def _retrieve_by_bm25(self, table_info: Dict[str, Dict], user_query: str) -> List[int]:
        """
        ä½¿ç”¨ BM25 ç®—æ³•è¿›è¡Œå…³é”®è¯åŒ¹é…æ£€ç´¢ã€‚

        Args:
            table_info (Dict[str, Dict]): è¡¨ç»“æ„
            user_query (str): ç”¨æˆ·æŸ¥è¯¢

        Returns:
            List[int]: æŒ‰ç›¸å…³æ€§æ’åºçš„ç´¢å¼•åˆ—è¡¨
        """
        if not user_query or not table_info:
            return list(range(len(table_info)))

        logger.info("ğŸ”„ æ‰§è¡Œ BM25 æ£€ç´¢...")
        self._corpus = [self._build_document(name, info) for name, info in table_info.items()]
        self._tokenized_corpus = [self._tokenize_text(doc) for doc in self._corpus]
        query_tokens = self._tokenize_text(user_query)

        bm25 = BM25Okapi(self._tokenized_corpus)
        doc_scores = bm25.get_scores(query_tokens)

        # å¢å¼ºï¼šè‹¥æŸ¥è¯¢è¯å‡ºç°åœ¨è¡¨æ³¨é‡Šä¸­ï¼Œåˆ™æå‡åˆ†æ•°
        enhanced_scores = doc_scores.copy()
        table_comments = [info.get("table_comment", "") for info in table_info.values()]
        for i, (comment, score) in enumerate(zip(table_comments, doc_scores)):
            if score <= 0:
                continue
            comment_tokens = self._tokenize_text(comment)
            overlap = set(query_tokens) & set(comment_tokens)
            if overlap:
                overlap_ratio = len(overlap) / len(set(query_tokens))
                enhanced_scores[i] += score * overlap_ratio * 1.5

        scored_indices = sorted(enumerate(enhanced_scores), key=lambda x: x[1], reverse=True)
        return [idx for idx, _ in scored_indices]

    @staticmethod
    def _rrf_fusion(bm25_indices: List[int], vector_indices: List[int], k: int = 60) -> List[int]:
        """
        ä½¿ç”¨ RRFï¼ˆReciprocal Rank Fusionï¼‰èåˆä¸¤ç§æ£€ç´¢ç»“æœã€‚

        Args:
            bm25_indices (List[int]): BM25 æ’åºç´¢å¼•
            vector_indices (List[int]): å‘é‡æ£€ç´¢æ’åºç´¢å¼•
            k (int): RRF å¸¸æ•°

        Returns:
            List[int]: èåˆåæ’åºçš„ç´¢å¼•åˆ—è¡¨
        """
        scores = {}
        for rank, idx in enumerate(bm25_indices):
            scores[idx] = scores.get(idx, 0) + 1 / (k + rank + 1)
        for rank, idx in enumerate(vector_indices):
            scores[idx] = scores.get(idx, 0) + 1 / (k + rank + 1)
        sorted_indices = sorted(scores.items(), key=lambda x: -x[1])
        return [idx for idx, _ in sorted_indices]

    def _rerank_with_dashscope(self, query: str, candidate_tables: Dict[str, Dict]) -> List[Tuple[str, float]]:
        """
        ä½¿ç”¨ DashScope GTE-Rerank-V2 å¯¹å€™é€‰è¡¨è¿›è¡Œé‡æ’åºã€‚

        Args:
            query (str): ç”¨æˆ·æŸ¥è¯¢
            candidate_tables (Dict[str, Dict]): å€™é€‰è¡¨åŠå…¶ä¿¡æ¯

        Returns:
            List[Tuple[str, float]]: (è¡¨å, ç›¸å…³æ€§åˆ†æ•°) åˆ—è¡¨ï¼ŒæŒ‰åˆ†æ•°é™åº
        """
        if not self.USE_RERANKER:
            logger.debug("â­ï¸ Reranker å·²ç¦ç”¨ï¼Œè·³è¿‡é‡æ’åº")
            return [(name, 1.0) for name in candidate_tables.keys()]

        try:
            documents = []
            name_to_text = {}
            for table_name, info in candidate_tables.items():
                doc_text = self._build_document(table_name, info)
                documents.append(doc_text)
                name_to_text[table_name] = doc_text

            if not documents:
                return []

            logger.info("ğŸ” è°ƒç”¨ GTE-Rerank-V2 è¿›è¡Œé‡æ’åº...")
            response = dashscope.TextReRank.call(
                api_key=MODEL_API_KEY,
                model=RERANK_MODEL_NAME,
                query=query,
                documents=documents,
                top_n=len(documents),
                return_documents=False,
            )

            if response.status_code == HTTPStatus.OK:
                results = []
                for item in response.output.results:
                    table_name = next(name for name, text in name_to_text.items() if text == documents[item.index])
                    results.append((table_name, item.relevance_score))
                results.sort(key=lambda x: x[1], reverse=True)
                logger.info("âœ… Rerank å®Œæˆ")
                return results
            else:
                logger.warning(f"âš ï¸ Rerank API è°ƒç”¨å¤±è´¥: {response.message}")
                return [(name, 1.0) for name in candidate_tables.keys()]

        except Exception as e:
            logger.error(f"âŒ Rerank è¿‡ç¨‹å‡ºé”™: {e}")
            return [(name, 1.0) for name in candidate_tables.keys()]

    def get_table_schema(self, state: AgentState) -> AgentState:
        """
        æ ¹æ®ç”¨æˆ·æŸ¥è¯¢ï¼Œé€šè¿‡æ··åˆæ£€ç´¢ç­›é€‰å‡ºæœ€ç›¸å…³çš„æ•°æ®åº“è¡¨ç»“æ„ã€‚

        Args:
            state (AgentState): å½“å‰çŠ¶æ€ï¼ŒåŒ…å« user_query

        Returns:
            AgentState: æ›´æ–°åçš„çŠ¶æ€ï¼ŒåŒ…å« db_info
        """
        try:
            logger.info("ğŸ” å¼€å§‹è·å–æ•°æ®åº“è¡¨ schema ä¿¡æ¯")
            all_table_info = self._fetch_all_table_info()

            user_query = state.get("user_query", "").strip()
            if not user_query:
                state["db_info"] = all_table_info
                logger.info(f"â„¹ï¸ æ— ç”¨æˆ·æŸ¥è¯¢ï¼Œè¿”å›å…¨éƒ¨ {len(all_table_info)} å¼ è¡¨")
                return state

            # åˆå§‹åŒ–å‘é‡ç´¢å¼•
            self._initialize_vector_index(all_table_info)

            # æ··åˆæ£€ç´¢
            logger.info("ğŸ” å¼€å§‹æ··åˆæ£€ç´¢ï¼šBM25 + å‘é‡æ£€ç´¢")
            bm25_top_indices = self._retrieve_by_bm25(all_table_info, user_query)
            logger.info(f"ğŸ“Š BM25æ£€ç´¢è¿”å› {len(bm25_top_indices)} ä¸ªç»“æœ")
            vector_top_indices = self._retrieve_by_vector(user_query, top_k=20)
            logger.info(f"ğŸ”— å‘é‡æ£€ç´¢è¿”å› {len(vector_top_indices)} ä¸ªç»“æœ")

            # è¿‡æ»¤ï¼šä»…ä¿ç•™åŒæ—¶åœ¨ BM25 å‰ 50 å’Œå‘é‡ç»“æœä¸­çš„è¡¨
            valid_bm25_set = set(bm25_top_indices[:50])
            candidate_indices = [idx for idx in vector_top_indices if idx in valid_bm25_set]
            logger.info(f"ğŸ¯ åˆæ­¥ç­›é€‰åä¿ç•™ {len(candidate_indices)} ä¸ªå€™é€‰è¡¨")

            if not candidate_indices:
                candidate_indices = bm25_top_indices[:4]  # é™çº§
                logger.info("âš ï¸ å€™é€‰è¡¨ä¸ºç©ºï¼Œé™çº§ä½¿ç”¨BM25å‰4ä¸ªç»“æœ")

            fused_indices = self._rrf_fusion(bm25_top_indices, candidate_indices, k=60)
            logger.info(f"ğŸ”„ RRFèåˆåå¾—åˆ° {len(fused_indices)} ä¸ªç»“æœ")

            # è¯„åˆ†ç­›é€‰
            selected_indices = []
            for idx in fused_indices:
                bm25_rank = bm25_top_indices.index(idx) + 1 if idx in bm25_top_indices else len(all_table_info) + 1
                vector_rank = (
                    vector_top_indices.index(idx) + 1 if idx in vector_top_indices else len(all_table_info) + 1
                )
                score = 1 / (60 + bm25_rank) + 1 / (60 + vector_rank)
                if score >= 0.01 and len(selected_indices) < 10:
                    selected_indices.append(idx)

            candidate_table_names = [self._table_names[i] for i in selected_indices]
            candidate_table_info = {name: all_table_info[name] for name in candidate_table_names}

            # é‡æ’åº
            reranked_results = self._rerank_with_dashscope(user_query, candidate_table_info)
            final_table_names = [name for name, _ in reranked_results][:4]  # å– top 4

            # æ„å»ºè¾“å‡º
            filtered_info = {name: all_table_info[name] for name in final_table_names}

            # æ‰“å°ç»“æœæ‘˜è¦
            print(f"\nğŸ” ç”¨æˆ·æŸ¥è¯¢: {user_query}")
            print("ğŸ“Š æ£€ç´¢ä¸æ’åºç»“æœ:")
            for i, (table_name, score) in enumerate(reranked_results):
                bm25_idx = self._table_names.index(table_name) if table_name in self._table_names else -1
                bm25_rank = bm25_top_indices.index(bm25_idx) + 1 if bm25_idx in bm25_top_indices else "-"
                vector_rank = vector_top_indices.index(bm25_idx) + 1 if bm25_idx in vector_top_indices else "-"
                print(
                    f"  {i + 1}. {table_name:<15} | BM25: {bm25_rank:>2} | Vector: {vector_rank:>2} | Rerank: {score:.3f}"
                )

            state["db_info"] = filtered_info
            logger.info(f"âœ… æœ€ç»ˆç­›é€‰å‡º {len(filtered_info)} ä¸ªç›¸å…³è¡¨: {list(filtered_info.keys())}")

        except Exception as e:
            logger.error(f"âŒ è·å–æ•°æ®åº“è¡¨ä¿¡æ¯å¤±è´¥: {e}", exc_info=True)
            state["db_info"] = {}
            state["execution_result"] = ExecutionResult(success=False, error="æ— æ³•è¿æ¥æ•°æ®åº“æˆ–è·å–å…ƒæ•°æ®")

        return state

    @staticmethod
    def execute_sql(state: AgentState) -> AgentState:
        """
        æ‰§è¡Œç”Ÿæˆçš„ SQL è¯­å¥ã€‚

        Args:
            state (AgentState): åŒ…å« generated_sql çš„çŠ¶æ€

        Returns:
            AgentState: æ›´æ–°æ‰§è¡Œç»“æœ
        """
        generated_sql = state.get("generated_sql", "").strip()
        if not generated_sql:
            error_msg = "SQL ä¸ºç©ºï¼Œæ— æ³•æ‰§è¡Œ"
            logger.warning(error_msg)
            state["execution_result"] = ExecutionResult(success=False, error=error_msg)
            return state

        logger.info("â–¶ï¸ æ‰§è¡Œ SQL è¯­å¥")
        try:
            with db_pool.get_session() as session:
                result = session.execute(text(generated_sql))
                result_data = result.fetchall()
                columns = result.keys()
                frame = pd.DataFrame(result_data, columns=columns)
                state["execution_result"] = ExecutionResult(success=True, data=frame.to_dict(orient="records"))
                logger.info(f"âœ… SQL æ‰§è¡ŒæˆåŠŸï¼Œè¿”å› {len(result_data)} æ¡è®°å½•")
        except Exception as e:
            error_msg = f"æ‰§è¡Œ SQL å¤±è´¥: {e}"
            logger.error(error_msg, exc_info=True)
            state["execution_result"] = ExecutionResult(success=False, error=str(e))
        return state

    @staticmethod
    def execute_correction_sql(state: AgentState) -> AgentState:
        """
        æ‰§è¡Œä¿®æ­£åçš„ SQL è¯­å¥ã€‚

        Args:
            state (AgentState): åŒ…å« correction_result çš„çŠ¶æ€

        Returns:
            AgentState: æ›´æ–°æ‰§è¡Œç»“æœ
        """
        correction_result = state.get("correction_result")
        if not correction_result or not hasattr(correction_result, "corrected_sql_query"):
            error_msg = "æ— ä¿®æ­£åçš„ SQL å¯æ‰§è¡Œ"
            logger.warning(error_msg)
            state["execution_result"] = ExecutionResult(success=False, error=error_msg)
            return state

        corrected_sql = getattr(correction_result, "corrected_sql_query", "").strip()
        if not corrected_sql:
            error_msg = "ä¿®æ­£åçš„ SQL ä¸ºç©º"
            logger.warning(error_msg)
            state["execution_result"] = ExecutionResult(success=False, error=error_msg)
            return state

        logger.info("ğŸ”§ æ‰§è¡Œä¿®æ­£åçš„ SQL è¯­å¥")
        try:
            with db_pool.get_session() as session:
                result = session.execute(text(corrected_sql))
                result_data = result.fetchall()
                columns = result.keys()
                frame = pd.DataFrame(result_data, columns=columns)
                state["execution_result"] = ExecutionResult(success=True, data=frame.to_dict(orient="records"))
                logger.info(f"âœ… ä¿®æ­£ SQL æ‰§è¡ŒæˆåŠŸï¼Œè¿”å› {len(result_data)} æ¡è®°å½•")
        except Exception as e:
            error_msg = f"æ‰§è¡Œä¿®æ­£ SQL å¤±è´¥: {e}"
            logger.error(error_msg, exc_info=True)
            state["execution_result"] = ExecutionResult(success=False, error=str(e))
        return state
