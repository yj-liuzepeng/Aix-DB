import warnings

from sqlalchemy import create_engine

from common.datasource_util import DatasourceConfigUtil, DatasourceConnectionUtil
from model import Datasource

warnings.filterwarnings("ignore", message=".*pkg_resources.*deprecated.*")

import json
import logging
import os
import re
import time
from typing import Dict, List, Tuple, Optional
from threading import Lock
from concurrent.futures import ThreadPoolExecutor

import faiss
import jieba
import numpy as np
import pandas as pd
import requests

from langfuse.openai import OpenAI
from rank_bm25 import BM25Okapi
from sqlalchemy.inspection import inspect
from sqlalchemy.sql.expression import text

from agent.text2sql.state.agent_state import AgentState, ExecutionResult
from model.db_connection_pool import get_db_pool
from model.db_models import TAiModel, TDsPermission, TDsRules
from model.datasource_models import DatasourceTable, DatasourceField
from agent.text2sql.permission.permission_retriever import get_user_permission_filters
from sqlalchemy import select

# æ—¥å¿—é…ç½®
logger = logging.getLogger(__name__)

# æ•°æ®åº“è¿æ¥æ± 
db_pool = get_db_pool()


# è¿”å›è¡¨æ•°é‡é…ç½®ï¼ˆå¯é…ç½®ï¼Œé»˜è®¤ 6 ä¸ªï¼‰
TABLE_RETURN_COUNT = int(os.getenv("TABLE_RETURN_COUNT", "6"))

# ç¼“å­˜é…ç½®
_table_info_cache: Dict[Tuple[int, Optional[int]], Tuple[Dict[str, Dict], float]] = {}
_cache_lock = Lock()
CACHE_TTL = int(os.getenv("TABLE_INFO_CACHE_TTL", "300"))  # ç¼“å­˜æœ‰æ•ˆæœŸï¼ˆç§’ï¼‰ï¼Œé»˜è®¤5åˆ†é’Ÿ


# åµŒå…¥æ¨¡å‹é…ç½®
def get_embedding_model_config():
    """
    è·å–åµŒå…¥æ¨¡å‹é…ç½®
    åªæŸ¥æ‰¾ Embedding ç±»å‹çš„æ¨¡å‹ï¼ˆmodel_type=2ï¼‰ï¼Œä¸å›é€€åˆ° LLM
    å¦‚æœæ²¡æœ‰é…ç½®ï¼Œè¿”å› Noneï¼ˆå°†ä½¿ç”¨ç¦»çº¿æ¨¡å‹ï¼‰
    """
    with db_pool.get_session() as session:
        # model_type: 2 -> Embedding
        model = session.query(TAiModel).filter(TAiModel.model_type == 2, TAiModel.default_model == True).first()
        
        if not model:
            # å°è¯•æŸ¥æ‰¾ä»»ä½• embedding æ¨¡å‹
            model = session.query(TAiModel).filter(TAiModel.model_type == 2).first()
        
        if not model:
            # æ²¡æœ‰æ‰¾åˆ°åœ¨çº¿æ¨¡å‹ï¼Œè¿”å› Noneï¼ˆå°†ä½¿ç”¨ç¦»çº¿æ¨¡å‹ï¼‰
            return None
        
        # å¤„ç† base_urlï¼Œç¡®ä¿åŒ…å«åè®®å‰ç¼€
        base_url = (model.api_domain or "").strip()
        if not base_url:
            logger.warning("è¡¨ç»“æ„æ£€ç´¢ä½¿ç”¨çš„ embedding æ¨¡å‹ API Domain ä¸ºç©ºï¼Œå°†ä½¿ç”¨ç¦»çº¿æ¨¡å‹")
            return None
        
        if not base_url.startswith(("http://", "https://")):
            # æœ¬åœ°åœ°å€é»˜è®¤ httpï¼Œå…¶å®ƒé»˜è®¤ https
            if base_url.startswith(("localhost", "127.0.0.1", "0.0.0.0")):
                base_url = f"http://{base_url}"
            else:
                base_url = f"https://{base_url}"
        
        return {"name": model.base_model, "api_key": model.api_key, "base_url": base_url}


# é‡æ’æ¨¡å‹é…ç½®
def get_rerank_model_config():
    with db_pool.get_session() as session:
        # model_type: 3 -> Rerank
        model = session.query(TAiModel).filter(TAiModel.model_type == 3, TAiModel.default_model == True).first()

        if not model:
            # Fallback
            model = session.query(TAiModel).filter(TAiModel.model_type == 3).first()

        if not model:
            return None

        return {"name": model.base_model, "api_key": model.api_key, "base_url": model.api_domain}


# å…¨å±€å˜é‡å ä½ï¼Œå®é™…ä½¿ç”¨æ—¶åŠ¨æ€è·å–æˆ–åœ¨ init ä¸­åˆå§‹åŒ–
# ä½†ä¸ºäº†ä¿æŒå…¼å®¹æ€§ï¼Œè¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ lazy initialization æˆ–è€… property


class DatabaseService:
    """
    æ”¯æŒæ··åˆæ£€ç´¢ï¼ˆBM25 + å‘é‡ï¼‰ä¸ç´¢å¼•æŒä¹…åŒ–çš„æ•°æ®åº“æœåŠ¡ã€‚
    æä¾›è¡¨ç»“æ„æ£€ç´¢ã€SQL æ‰§è¡Œã€é”™è¯¯ä¿®æ­£ SQL æ‰§è¡Œç­‰åŠŸèƒ½ã€‚
    """

    def __init__(self, datasource_id: int = None):
        self._engine = None
        self._datasource_id = datasource_id
        self._datasource = None
        
        if datasource_id:
            try:
                with db_pool.get_session() as session:
                    ds = session.query(Datasource).filter(Datasource.id == datasource_id).first()
                    if ds:
                        self._datasource = ds
                        config = DatasourceConfigUtil.decrypt_config(ds.configuration)
                        uri = DatasourceConnectionUtil.build_connection_uri(ds.type, config)
                        self._engine = create_engine(uri)
                        logger.info(f"Initialized DatabaseService with datasource_id: {datasource_id}")
            except Exception as e:
                logger.error(f"Failed to initialize datasource {datasource_id}: {e}")

        if not self._engine:
            self._engine = db_pool.get_engine()

        self._faiss_index: Optional[faiss.Index] = None
        self._table_names: List[str] = []
        self._corpus: List[str] = []
        self._tokenized_corpus: List[List[str]] = []
        self._index_initialized: bool = False
        self.USE_RERANKER: bool = True  # æ˜¯å¦å¯ç”¨é‡æ’åºå™¨

        # Initialize clients lazily or now
        emb_config = get_embedding_model_config()
        if emb_config:
            # ä½¿ç”¨åœ¨çº¿ embedding æ¨¡å‹
            try:
                self.embedding_model_name = emb_config["name"]
                self.embedding_client = OpenAI(api_key=emb_config["api_key"] or "empty", base_url=emb_config["base_url"])
                self.use_local_embedding = False
                logger.info(f"âœ… ä½¿ç”¨åœ¨çº¿ embedding æ¨¡å‹: {self.embedding_model_name}")
            except Exception as e:
                logger.error(f"åˆå§‹åŒ–åœ¨çº¿åµŒå…¥æ¨¡å‹å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨ç¦»çº¿æ¨¡å‹")
                self.embedding_client = None
                self.use_local_embedding = True
        else:
            # æ²¡æœ‰é…ç½®åœ¨çº¿æ¨¡å‹ï¼Œä½¿ç”¨ç¦»çº¿æ¨¡å‹
            logger.info("æœªé…ç½®åœ¨çº¿ embedding æ¨¡å‹ï¼Œå°†ä½¿ç”¨ç¦»çº¿ CPU æ¨¡å‹")
            self.embedding_client = None
            self.embedding_model_name = None
            self.use_local_embedding = True

        try:
            rerank_config = get_rerank_model_config()
            if rerank_config:
                self.rerank_model_name = rerank_config["name"]
                self.rerank_api_key = rerank_config["api_key"]
                self.rerank_base_url = rerank_config["base_url"]
                self.USE_RERANKER = True
            else:
                self.USE_RERANKER = False
                logger.warning("æœªé…ç½®é‡æ’æ¨¡å‹ï¼Œé‡æ’åŠŸèƒ½å°†è¢«ç¦ç”¨")
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–é‡æ’æ¨¡å‹å¤±è´¥: {e}")
            self.USE_RERANKER = False

    @staticmethod
    def _tokenize_text(text_str: str) -> List[str]:
        """
        å¯¹ä¸­æ–‡/è‹±æ–‡æ–‡æœ¬è¿›è¡Œåˆ†è¯ï¼Œè¿‡æ»¤æ ‡ç‚¹ç¬¦å·ã€‚
        """
        filtered_text = re.sub(r"[^\u4e00-\u9fa5a-zA-Z0-9]", " ", text_str)
        tokens = jieba.lcut(filtered_text, cut_all=False)
        return [token.strip() for token in tokens if token.strip()]

    def _get_table_comment(self, table_name: str) -> str:
        """
        ä» information_schema ä¸­è·å–æŒ‡å®šè¡¨çš„æ³¨é‡Šã€‚
        """
        try:
            query = text(
                """
                SELECT table_comment
                FROM information_schema.tables
                WHERE table_schema = DATABASE()
                  AND table_name = :table_name;
                """
            )
            with self._engine.connect() as conn:
                result = conn.execute(query, {"table_name": table_name})
                row = result.fetchone()
                return (row[0] or "").strip()
        except Exception as e:
            logger.warning(f"âš ï¸ è·å–è¡¨ {table_name} æ³¨é‡Šå¤±è´¥: {e}")
            return ""

    @staticmethod
    def _build_document(table_name: str, table_info: dict) -> str:
        """
        æ„å»ºç”¨äºæ£€ç´¢çš„æ–‡æ¡£æ–‡æœ¬ï¼ˆè¡¨å + æ³¨é‡Š + å­—æ®µå + å­—æ®µæ³¨é‡Šï¼‰ã€‚
        """
        parts = [table_name]
        if table_info.get("table_comment"):
            parts.append(table_info["table_comment"])
        for col_name, col_info in table_info.get("columns", {}).items():
            parts.append(col_name)
            if col_info.get("comment"):
                parts.append(col_info["comment"])
        return " ".join(parts)

    def _fetch_all_table_info(self, user_id: Optional[int] = None, use_cache: bool = True) -> Dict[str, Dict]:
        """
        è·å–æ•°æ®åº“ä¸­æ‰€æœ‰è¡¨çš„ç»“æ„ä¿¡æ¯ï¼ˆå¸¦æƒé™è¿‡æ»¤å’Œç¼“å­˜ï¼‰ã€‚
        
        Args:
            user_id: ç”¨æˆ·IDï¼Œç”¨äºæƒé™è¿‡æ»¤ï¼ˆç®¡ç†å‘˜ä¸åº”ç”¨æƒé™è¿‡æ»¤ï¼‰
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            
        Returns:
            è¡¨ä¿¡æ¯å­—å…¸
        """
        from common.permission_util import is_admin
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = (self._datasource_id or 0, user_id)
        if use_cache:
            with _cache_lock:
                if cache_key in _table_info_cache:
                    cached_data, cached_time = _table_info_cache[cache_key]
                    if time.time() - cached_time < CACHE_TTL:
                        logger.debug(f"âœ… ä½¿ç”¨ç¼“å­˜çš„è¡¨ç»“æ„ä¿¡æ¯ (datasource_id={self._datasource_id}, user_id={user_id})")
                        return cached_data
        
        start_time = time.time()
        inspector = inspect(self._engine)
        table_names = inspector.get_table_names()
        logger.info(f"ğŸ” å¼€å§‹åŠ è½½ {len(table_names)} å¼ è¡¨çš„ schema ä¿¡æ¯...")

        # è·å–åˆ—æƒé™é…ç½®ï¼ˆé›†æˆå®Œæ•´çš„æƒé™ç³»ç»Ÿï¼‰
        column_permissions = {}
        if user_id and not is_admin(user_id) and self._datasource_id:
            try:
                with db_pool.get_session() as session:
                    # è·å–è¯¥æ•°æ®æºä¸‹æ‰€æœ‰è¡¨
                    tables = session.query(DatasourceTable).filter(
                        DatasourceTable.ds_id == self._datasource_id,
                        DatasourceTable.table_name.in_(table_names)
                    ).all()
                    
                    # è·å–æ‰€æœ‰è§„åˆ™
                    rules_stmt = select(TDsRules).where(TDsRules.enable == True)
                    rules = session.execute(rules_stmt).scalars().all()
                    
                    for table in tables:
                        allowed_fields = set()
                        
                        # å¦‚æœæœ‰è§„åˆ™ï¼ŒæŸ¥è¯¢åˆ—æƒé™é…ç½®
                        if rules:
                            permissions_stmt = select(TDsPermission).where(
                                TDsPermission.table_id == table.id,
                                TDsPermission.type == 'column',
                                TDsPermission.enable == True
                            )
                            column_perms = session.execute(permissions_stmt).scalars().all()
                            
                            if column_perms:
                                # æ£€æŸ¥æƒé™æ˜¯å¦ä¸ç”¨æˆ·åŒ¹é…
                                matching_permissions = []
                                for permission in column_perms:
                                    for rule in rules:
                                        perm_ids = []
                                        if rule.permission_list:
                                            try:
                                                perm_ids = json.loads(rule.permission_list)
                                            except:
                                                pass
                                        
                                        user_ids = []
                                        if rule.user_list:
                                            try:
                                                user_ids = json.loads(rule.user_list)
                                            except:
                                                pass
                                        
                                        if perm_ids and user_ids:
                                            if permission.id in perm_ids and (
                                                user_id in user_ids or str(user_id) in user_ids
                                            ):
                                                matching_permissions.append(permission)
                                                break
                                
                                # è§£æåˆ—æƒé™é…ç½®
                                for perm in matching_permissions:
                                    if perm.permissions:
                                        try:
                                            perm_config = json.loads(perm.permissions)
                                            if isinstance(perm_config, list):
                                                for field_perm in perm_config:
                                                    if field_perm.get("enable", False):
                                                        field_name = field_perm.get("field_name")
                                                        if field_name:
                                                            allowed_fields.add(field_name)
                                        except Exception as e:
                                            logger.debug(f"è§£æåˆ—æƒé™é…ç½®å¤±è´¥: {e}, permission_id={perm.id}")
                        
                        # å¦‚æœæ²¡æœ‰åŒ¹é…çš„æƒé™é…ç½®ï¼Œä½¿ç”¨ checked å­—æ®µä½œä¸ºåŸºç¡€
                        if not allowed_fields:
                            fields = session.query(DatasourceField).filter(
                                DatasourceField.ds_id == self._datasource_id,
                                DatasourceField.table_id == table.id,
                                DatasourceField.checked == True
                            ).all()
                            allowed_fields = {field.field_name for field in fields}
                        
                        if allowed_fields:
                            column_permissions[table.table_name] = allowed_fields
                            
            except Exception as e:
                logger.warning(f"âš ï¸ è·å–åˆ—æƒé™å¤±è´¥: {e}", exc_info=True)

        table_info = {}
        for table_name in table_names:
            try:
                columns = {}
                for col in inspector.get_columns(table_name):
                    # æƒé™è¿‡æ»¤ï¼šå¦‚æœé…ç½®äº†åˆ—æƒé™ï¼Œåªè¿”å›æœ‰æƒé™çš„å­—æ®µ
                    if table_name in column_permissions:
                        if col["name"] not in column_permissions[table_name]:
                            continue
                    
                    columns[col["name"]] = {
                        "type": str(col["type"]),
                        "comment": str(col["comment"] or ""),
                    }

                # å¦‚æœè¿‡æ»¤åæ²¡æœ‰å­—æ®µï¼Œè·³è¿‡è¯¥è¡¨
                if not columns:
                    logger.debug(f"âš ï¸ è¡¨ {table_name} æ— å¯ç”¨å­—æ®µï¼ˆæƒé™è¿‡æ»¤åï¼‰ï¼Œè·³è¿‡")
                    continue

                foreign_keys = [
                    f"{fk['constrained_columns'][0]} -> {fk['referred_table']}.{fk['referred_columns'][0]}"
                    for fk in inspector.get_foreign_keys(table_name)
                ]

                table_comment = self._get_table_comment(table_name)

                table_info[table_name] = {
                    "columns": columns,
                    "foreign_keys": foreign_keys,
                    "table_comment": table_comment,
                }
            except Exception as e:
                logger.error(f"âŒ è¯»å–è¡¨ {table_name} ç»“æ„å¤±è´¥: {e}")

        elapsed = time.time() - start_time
        logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(table_info)} å¼ è¡¨ï¼Œè€—æ—¶ {elapsed:.2f}s")
        
        # æ›´æ–°ç¼“å­˜
        if use_cache:
            with _cache_lock:
                _table_info_cache[cache_key] = (table_info, time.time())
        
        return table_info


    def _get_precomputed_embeddings(self, table_info: Dict[str, Dict]) -> Tuple[Optional[np.ndarray], List[str], List[str]]:
        """
        å°è¯•ä»æ•°æ®åº“è·å–é¢„è®¡ç®—çš„ embeddingã€‚
        ä»…ä» t_datasource_table.embedding å­—æ®µè¯»å–ï¼Œä¸åšä»»ä½•å®æ—¶è®¡ç®—ã€‚

        Returns:
            (é¢„è®¡ç®—çš„ embedding æ•°ç»„, æœ‰é¢„è®¡ç®— embedding çš„è¡¨ååˆ—è¡¨, éœ€è¦è®¡ç®—çš„è¡¨ååˆ—è¡¨)
        """
        if not self._datasource_id:
            return None, [], list(table_info.keys())
        
        try:
            with db_pool.get_session() as session:
                # æŸ¥è¯¢æ•°æ®æºä¸‹çš„æ‰€æœ‰è¡¨
                tables = session.query(DatasourceTable).filter(
                    DatasourceTable.ds_id == self._datasource_id,
                    DatasourceTable.table_name.in_(list(table_info.keys()))
                ).all()
                
                # æ„å»ºè¡¨ååˆ°è¡¨çš„æ˜ å°„
                table_map = {table.table_name: table for table in tables}
                
                # æ”¶é›†æœ‰é¢„è®¡ç®— embedding çš„è¡¨
                precomputed_embeddings = []
                precomputed_table_names = []
                missing_table_names = []
                
                for table_name, info in table_info.items():
                    table = table_map.get(table_name)
                    # æ£€æŸ¥æ˜¯å¦æœ‰ embedding å­—æ®µï¼ˆé€šè¿‡ hasattr æ£€æŸ¥ï¼Œé¿å…å­—æ®µä¸å­˜åœ¨æ—¶æŠ¥é”™ï¼‰
                    if table and hasattr(table, 'embedding') and table.embedding:
                        try:
                            embedding_vec = json.loads(table.embedding)
                            if isinstance(embedding_vec, list) and len(embedding_vec) > 0:
                                precomputed_embeddings.append(embedding_vec)
                                precomputed_table_names.append(table_name)
                            else:
                                missing_table_names.append(table_name)
                        except Exception as e:
                            logger.debug(f"è§£æè¡¨ {table_name} çš„ embedding å¤±è´¥: {e}")
                            missing_table_names.append(table_name)
                    else:
                        missing_table_names.append(table_name)
                
                if precomputed_embeddings:
                    embeddings_array = np.array(precomputed_embeddings).astype("float32")
                    faiss.normalize_L2(embeddings_array)
                    logger.info(f"âœ… ä»æ•°æ®åº“åŠ è½½äº† {len(precomputed_embeddings)} ä¸ªé¢„è®¡ç®—çš„ embedding")
                    return embeddings_array, precomputed_table_names, missing_table_names
                else:
                    return None, [], missing_table_names
                    
        except Exception as e:
            logger.warning(f"âš ï¸ è·å–é¢„è®¡ç®— embedding å¤±è´¥: {e}")
            return None, [], list(table_info.keys())

    def _create_embeddings_with_dashscope(self, texts: List[str]) -> np.ndarray:
        """
        ç”Ÿæˆæ–‡æœ¬åµŒå…¥å‘é‡ã€‚
        ä¼˜å…ˆä½¿ç”¨åœ¨çº¿æ¨¡å‹ï¼Œå¦‚æœæ²¡æœ‰é…ç½®åˆ™ä½¿ç”¨ç¦»çº¿æ¨¡å‹ã€‚

        æ³¨æ„ï¼šè¯¥æ–¹æ³•ä¸åœ¨åœ¨çº¿æ£€ç´¢è·¯å¾„ä¸­è°ƒç”¨ï¼Œä»…ç”¨äºç¦»çº¿é¢„è®¡ç®—å·¥å…·
        æˆ–å¼ºåˆ¶é‡å»ºç´¢å¼•ç­‰ç®¡ç†åœºæ™¯ä¸­ä½¿ç”¨ã€‚
        """
        if self.use_local_embedding or not self.embedding_client:
            # ä½¿ç”¨ç¦»çº¿æ¨¡å‹
            from common.local_embedding import generate_embedding_local_sync
            logger.info("ğŸ–¥ï¸ ä½¿ç”¨ç¦»çº¿ CPU æ¨¡å‹ç”Ÿæˆ embedding...")
            start_time = time.time()
            embeddings = []
            embedding_dim = None  # åŠ¨æ€è·å–ç»´åº¦
            
            for doc in texts:
                try:
                    embedding = generate_embedding_local_sync(doc)
                    if embedding:
                        if embedding_dim is None:
                            embedding_dim = len(embedding)
                        embeddings.append(embedding)
                    else:
                        logger.warning(f"âš ï¸ ç¦»çº¿æ¨¡å‹ç”Ÿæˆ embedding å¤±è´¥ ({doc[:30]}...)ï¼Œä½¿ç”¨é›¶å‘é‡")
                        if embedding_dim is None:
                            embedding_dim = 768  # é»˜è®¤ç»´åº¦
                        embeddings.append([0.0] * embedding_dim)
                except Exception as e:
                    logger.error(f"âŒ ç¦»çº¿æ¨¡å‹åµŒå…¥ç”Ÿæˆå¤±è´¥ ({doc[:30]}...): {e}")
                    if embedding_dim is None:
                        embedding_dim = 768  # é»˜è®¤ç»´åº¦
                    embeddings.append([0.0] * embedding_dim)
            
            if not embeddings:
                logger.error("âŒ æ‰€æœ‰ embedding ç”Ÿæˆéƒ½å¤±è´¥")
                return np.array([])
            
            embeddings = np.array(embeddings).astype("float32")
            faiss.normalize_L2(embeddings)
            logger.info(f"âœ… ç¦»çº¿æ¨¡å‹åµŒå…¥ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶ {time.time() - start_time:.2f}sï¼Œç»´åº¦: {embedding_dim}")
            return embeddings
        
        # ä½¿ç”¨åœ¨çº¿æ¨¡å‹
        logger.info(f"ğŸŒ è°ƒç”¨åœ¨çº¿åµŒå…¥æ¨¡å‹ {self.embedding_model_name}...")
        start_time = time.time()
        embeddings = []
        for doc in texts:
            try:
                response = self.embedding_client.embeddings.create(model=self.embedding_model_name, input=doc)
                embeddings.append(response.data[0].embedding)
            except Exception as e:
                logger.error(f"âŒ åœ¨çº¿æ¨¡å‹åµŒå…¥ç”Ÿæˆå¤±è´¥ ({doc[:30]}...): {e}")
                embeddings.append(np.zeros(1024))  # å ä½ç¬¦

        embeddings = np.array(embeddings).astype("float32")
        faiss.normalize_L2(embeddings)
        logger.info(f"âœ… åœ¨çº¿æ¨¡å‹åµŒå…¥ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶ {time.time() - start_time:.2f}s")
        return embeddings

    def _initialize_vector_index(self, table_info: Dict[str, Dict]):
        """
        åˆå§‹åŒ– FAISS å‘é‡ç´¢å¼•ï¼šä»æ•°æ®åº“è¯»å–é¢„è®¡ç®—çš„ embedding å¹¶æ„å»ºå†…å­˜ç´¢å¼•ã€‚
        ä»…ä½¿ç”¨é¢„è®¡ç®—çš„ embeddingï¼Œä¸åœ¨æ£€ç´¢æ—¶åšå®æ—¶è®¡ç®—ã€‚
        """
        if self._index_initialized:
            return

        # æ„å»ºæ–°ç´¢å¼•
        logger.info("ğŸ—ï¸ å¼€å§‹æ„å»ºå‘é‡ç´¢å¼•ï¼ˆä»æ•°æ®åº“è¯»å– embeddingï¼‰...")
        start_time = time.time()

        # è®°å½•æ‰€æœ‰è¡¨åå’Œè¯­æ–™ï¼ˆç”¨äº BM25 ç­‰ï¼‰
        self._table_names = list(table_info.keys())
        self._corpus = [self._build_document(name, info) for name, info in table_info.items()]

        # ä»æ•°æ®åº“è·å–é¢„è®¡ç®—çš„ embeddingï¼ˆä¸ä¼šåšä»»ä½•å®æ—¶è®¡ç®—ï¼‰
        precomputed_embeddings, precomputed_table_names, missing_table_names = self._get_precomputed_embeddings(
            table_info
        )

        # å¦‚æœæ²¡æœ‰ä»»ä½•é¢„è®¡ç®— embeddingï¼Œåˆ™ç¦ç”¨å‘é‡ç´¢å¼•ï¼ˆä»…ä½¿ç”¨ BM25ï¼‰
        if precomputed_embeddings is None or len(precomputed_table_names) == 0:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•é¢„è®¡ç®—çš„è¡¨ç»“æ„ embeddingï¼Œå‘é‡æ£€ç´¢å°†è¢«ç¦ç”¨ï¼Œä»…ä½¿ç”¨ BM25")
            self._faiss_index = None
            self._index_initialized = True
            return

        # å¦‚æœå­˜åœ¨ç¼ºå¤±çš„ embeddingï¼Œä¸ºé¿å…ç´¢å¼•å’Œè¡¨é¡ºåºä¸ä¸€è‡´ï¼Œè¿™é‡Œç›´æ¥ç¦ç”¨å‘é‡æ£€ç´¢
        if len(missing_table_names) > 0:
            logger.warning(
                f"âš ï¸ å…±æœ‰ {len(missing_table_names)} å¼ è¡¨ç¼ºå°‘é¢„è®¡ç®— embeddingï¼Œ"
                "ä¸ºä¿è¯ç´¢å¼•ä¸è¡¨é¡ºåºä¸€è‡´ï¼Œæœ¬æ¬¡ç¦ç”¨å‘é‡æ£€ç´¢ï¼Œä»…ä½¿ç”¨ BM25"
            )
            self._faiss_index = None
            self._index_initialized = True
            return

        # æ­¤æ—¶è¯´æ˜æ‰€æœ‰è¡¨éƒ½å­˜åœ¨é¢„è®¡ç®— embeddingï¼Œé¡ºåºä¸ self._table_names ä¸€è‡´
        embeddings = precomputed_embeddings

        if embeddings.size == 0:
            logger.error("âŒ æ— æ³•ç”ŸæˆåµŒå…¥ï¼Œç´¢å¼•æ„å»ºå¤±è´¥")
            return

        # åˆå§‹åŒ– FAISS ç´¢å¼•ï¼ˆä»…åœ¨å†…å­˜ä¸­ï¼‰
        dimension = embeddings.shape[1]
        self._faiss_index = faiss.IndexFlatIP(dimension)  # å†…ç§¯ = ä½™å¼¦ç›¸ä¼¼åº¦
        self._faiss_index.add(embeddings)

        elapsed = time.time() - start_time
        logger.info(f"ğŸ‰ å‘é‡ç´¢å¼•æ„å»ºå®Œæˆï¼Œå…± {len(self._table_names)} å¼ è¡¨ï¼Œè€—æ—¶ {elapsed:.2f}s")
        self._index_initialized = True

    def _retrieve_by_vector(self, query: str, top_k: int = 10) -> List[int]:
        """
        ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦æ£€ç´¢æœ€ç›¸å…³çš„è¡¨ã€‚
        ä¼˜å…ˆä½¿ç”¨åœ¨çº¿æ¨¡å‹ï¼Œå¦‚æœæ²¡æœ‰é…ç½®åˆ™ä½¿ç”¨ç¦»çº¿æ¨¡å‹ã€‚
        """
        if not self._faiss_index:
            logger.error("âŒ å‘é‡ç´¢å¼•æœªåˆå§‹åŒ–")
            return []

        try:
            # ç”ŸæˆæŸ¥è¯¢å‘é‡
            if self.use_local_embedding or not self.embedding_client:
                # ä½¿ç”¨ç¦»çº¿æ¨¡å‹
                from common.local_embedding import generate_embedding_local_sync
                embedding = generate_embedding_local_sync(query)
                if not embedding:
                    logger.warning("âš ï¸ ç¦»çº¿æ¨¡å‹ç”Ÿæˆ embedding å¤±è´¥ï¼Œè·³è¿‡å‘é‡æ£€ç´¢")
                    return []
                query_vec = np.array([embedding]).astype("float32")
            else:
                # ä½¿ç”¨åœ¨çº¿æ¨¡å‹
                response = self.embedding_client.embeddings.create(model=self.embedding_model_name, input=query)
                query_vec = np.array([response.data[0].embedding]).astype("float32")
            
            # æ£€æŸ¥ç»´åº¦æ˜¯å¦åŒ¹é…
            query_dim = query_vec.shape[1]
            index_dim = self._faiss_index.d
            if query_dim != index_dim:
                logger.error(
                    f"âŒ å‘é‡ç»´åº¦ä¸åŒ¹é…ï¼šæŸ¥è¯¢å‘é‡ç»´åº¦={query_dim}ï¼Œç´¢å¼•ç»´åº¦={index_dim}ã€‚"
                    f"è¿™å¯èƒ½æ˜¯å› ä¸ºç´¢å¼•ä½¿ç”¨çš„æ˜¯åœ¨çº¿æ¨¡å‹çš„ embeddingï¼Œè€ŒæŸ¥è¯¢ä½¿ç”¨çš„æ˜¯ç¦»çº¿æ¨¡å‹ã€‚"
                    f"å»ºè®®ï¼šé‡æ–°è®¡ç®—è¡¨çš„ embedding æˆ–ä½¿ç”¨ç›¸åŒçš„æ¨¡å‹ã€‚"
                )
                return []
            
            faiss.normalize_L2(query_vec)
            _, indices = self._faiss_index.search(query_vec, top_k)
            return indices[0].tolist()
        except Exception as e:
            logger.error(f"âŒ å‘é‡æ£€ç´¢å¤±è´¥: {e}", exc_info=True)
            return []

    def _retrieve_by_bm25(self, table_info: Dict[str, Dict], user_query: str) -> List[int]:
        """
        ä½¿ç”¨ BM25 ç®—æ³•è¿›è¡Œå…³é”®è¯åŒ¹é…æ£€ç´¢ã€‚
        """
        if not user_query or not table_info:
            return list(range(len(table_info)))

        logger.info("ğŸ”„ æ‰§è¡Œ BM25 æ£€ç´¢...")
        self._corpus = [self._build_document(name, info) for name, info in table_info.items()]
        self._tokenized_corpus = [self._tokenize_text(doc) for doc in self._corpus]
        query_tokens = self._tokenize_text(user_query)

        bm25 = BM25Okapi(self._tokenized_corpus)
        doc_scores = bm25.get_scores(query_tokens)

        # å¢å¼ºï¼šè‹¥æŸ¥è¯¢è¯å‡ºç°åœ¨è¡¨æ³¨é‡Šä¸­ï¼Œåˆ™æå‡åˆ†æ•°
        enhanced_scores = doc_scores.copy()
        table_comments = [info.get("table_comment", "") for info in table_info.values()]
        for i, (comment, score) in enumerate(zip(table_comments, doc_scores)):
            if score <= 0:
                continue
            comment_tokens = self._tokenize_text(comment)
            overlap = set(query_tokens) & set(comment_tokens)
            if overlap:
                overlap_ratio = len(overlap) / len(set(query_tokens))
                enhanced_scores[i] += score * overlap_ratio * 1.5

        scored_indices = sorted(enumerate(enhanced_scores), key=lambda x: x[1], reverse=True)
        return [idx for idx, _ in scored_indices]

    @staticmethod
    def _rrf_fusion(bm25_indices: List[int], vector_indices: List[int], k: int = 60) -> List[int]:
        """
        ä½¿ç”¨ RRFï¼ˆReciprocal Rank Fusionï¼‰èåˆä¸¤ç§æ£€ç´¢ç»“æœã€‚
        """
        scores = {}
        for rank, idx in enumerate(bm25_indices):
            scores[idx] = scores.get(idx, 0) + 1 / (k + rank + 1)
        for rank, idx in enumerate(vector_indices):
            scores[idx] = scores.get(idx, 0) + 1 / (k + rank + 1)
        sorted_indices = sorted(scores.items(), key=lambda x: -x[1])
        return [idx for idx, _ in sorted_indices]

    def _rerank_with_dashscope(self, query: str, candidate_tables: Dict[str, Dict]) -> List[Tuple[str, float]]:
        """
        ä½¿ç”¨ DashScope é‡æ’ API å¯¹å€™é€‰è¡¨è¿›è¡Œé‡æ’åºã€‚
        """
        if not self.USE_RERANKER:
            logger.debug("â­ï¸ Reranker å·²ç¦ç”¨æˆ–é…ç½®ä¸å®Œæ•´ï¼Œè·³è¿‡é‡æ’åº")
            return [(name, 1.0) for name in candidate_tables.keys()]

        try:
            documents = []
            name_to_text = {}
            for table_name, info in candidate_tables.items():
                doc_text = self._build_document(table_name, info)
                documents.append(doc_text)
                name_to_text[table_name] = doc_text

            if not documents:
                return []

            logger.info(f"ğŸ” è°ƒç”¨é‡æ’æ¨¡å‹ {self.rerank_model_name} è¿›è¡Œé‡æ’åº...")

            # æ ¹æ®APIç±»å‹é€‰æ‹©ä¸åŒçš„è¯·æ±‚ç»“æ„
            if "aliyuncs" in self.rerank_base_url or "Qwen" in self.rerank_model_name:
                # é˜¿é‡Œäº‘ DashScope æ ¼å¼
                payload = {
                    "model": self.rerank_model_name,
                    "input": {"query": query, "documents": documents},
                    "parameters": {"top_n": len(documents), "return_documents": False},
                }
            else:
                # å…¶ä»–æ ¼å¼ï¼ˆå¦‚æœ¬åœ°æ¨¡å‹æˆ–é€šç”¨rerank APIï¼‰
                payload = {"query": query, "documents": documents}

            # è®¾ç½®è¯·æ±‚å¤´
            headers = {"Authorization": f"Bearer {self.rerank_api_key}", "Content-Type": "application/json"}

            # è°ƒç”¨é‡æ’ API
            response = requests.post(self.rerank_base_url, headers=headers, json=payload, timeout=30)

            # æ£€æŸ¥å“åº”çŠ¶æ€
            if response.status_code != 200:
                logger.warning(f"âš ï¸ Rerank API è°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")
                return [(name, 1.0) for name in candidate_tables.keys()]

            # è§£æå“åº”
            result_data = response.json()

            # æ ¹æ®APIç±»å‹è§£æå“åº”
            if "aliyuncs" in self.rerank_base_url or "Qwen" in self.rerank_model_name:
                # é˜¿é‡Œäº‘æ ¼å¼å“åº”
                if "output" in result_data and "results" in result_data["output"]:
                    results = []
                    for item in result_data["output"]["results"]:
                        idx = item["index"]
                        score = item["relevance_score"]
                        table_name = next(name for name, text in name_to_text.items() if text == documents[idx])
                        results.append((table_name, score))

                    results.sort(key=lambda x: x[1], reverse=True)
                    logger.info("âœ… Rerank å®Œæˆ")
                    return results
            else:
                # é€šç”¨æ ¼å¼å“åº” - å‡è®¾ç›´æ¥è¿”å›æ’åºç»“æœ
                if "results" in result_data:
                    results = []
                    for item in result_data["results"]:
                        if "index" in item and "relevance_score" in item:  # ä½¿ç”¨relevance_score
                            idx = item["index"]
                            score = item["relevance_score"]  # ä½¿ç”¨relevance_scoreå­—æ®µ
                            # ä»documentå¯¹è±¡ä¸­æå–æ–‡æœ¬
                            if "document" in item and "text" in item["document"]:
                                doc_text = item["document"]["text"]
                                table_name = next(name for name, text in name_to_text.items() if text == doc_text)
                            else:
                                table_name = next(name for name, text in name_to_text.items() if text == documents[idx])
                            results.append((table_name, score))
                    results.sort(key=lambda x: x[1], reverse=True)
                    logger.info("âœ… Rerank å®Œæˆ")
                    return results
                elif isinstance(result_data, list):
                    # å‡è®¾ç›´æ¥è¿”å›äº†æ’åºåçš„ç´¢å¼•åˆ—è¡¨
                    results = []
                    for i, item in enumerate(result_data):
                        if isinstance(item, dict) and "index" in item:
                            idx = item["index"]
                            score = item.get("score", 1.0 - i * 0.01)  # é»˜è®¤åˆ†æ•°é€’å‡
                            table_name = next(name for name, text in name_to_text.items() if text == documents[idx])
                            results.append((table_name, score))
                    logger.info("âœ… Rerank å®Œæˆ")
                    return results

            logger.warning("âš ï¸ Rerank API è¿”å›æ ¼å¼å¼‚å¸¸")
            return [(name, 1.0) for name in candidate_tables.keys()]

        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ Rerank API è¯·æ±‚å¤±è´¥: {e}")
            return [(name, 1.0) for name in candidate_tables.keys()]
        except Exception as e:
            logger.error(f"âŒ Rerank è¿‡ç¨‹å‡ºé”™: {e}")
            return [(name, 1.0) for name in candidate_tables.keys()]

    def supplement_related_tables(
        self,
        selected_table_names: List[str],
        all_table_info: Dict[str, Dict],
    ) -> List[str]:
        """

        - è¡¨èŠ‚ç‚¹: {"id": 15, "shape": "er-rect", "attrs": {"text": {"text": "t_products"}}, ...}
        - å…³ç³»è¾¹: {"shape": "edge", "source": {"cell": 15, "port": "135"}, "target": {"cell": 14, "port": "128"}}

        å…¶ä¸­ edge.source/target.cell ä½¿ç”¨çš„æ˜¯è¡¨è®°å½•ä¸»é”® IDï¼ˆå¯¹åº” DatasourceTable.idï¼‰ã€‚

        Args:
            selected_table_names: å·²é€‰ä¸­çš„è¡¨ååˆ—è¡¨ï¼ˆæ¥è‡ªæ£€ç´¢é˜¶æ®µè¿”å›çš„ db_info.keys()ï¼‰
            all_table_info: å½“å‰æ•°æ®æºä¸‹æ‰€æœ‰å¯ç”¨è¡¨çš„ä¿¡æ¯ dictï¼ˆç”¨äºè¿‡æ»¤è¡¥å……è¡¨æ˜¯å¦åœ¨æƒé™èŒƒå›´å†…ï¼‰

        Returns:
            æ‰©å±•åçš„è¡¨ååˆ—è¡¨ï¼ˆåŒ…å«åŸå§‹è¡¨å’Œé€šè¿‡è¡¨å…³ç³»è¡¥å……çš„å…³è”è¡¨ï¼‰
        """
        if not self._datasource_id or not selected_table_names:
            return selected_table_names

        try:
            with db_pool.get_session() as session:
                datasource = session.query(Datasource).filter(
                    Datasource.id == self._datasource_id
                ).first()
                if not datasource or not datasource.table_relation:
                    return selected_table_names

                relations = datasource.table_relation
                if not isinstance(relations, list):
                    return selected_table_names

                # èŠ‚ç‚¹å’Œè¾¹
                table_nodes = [
                    r for r in relations if r.get("shape") in ("er-rect", "rect")
                ]
                edges = [r for r in relations if r.get("shape") == "edge"]
                if not edges:
                    return selected_table_names

                # æŸ¥è¯¢è¯¥æ•°æ®æºä¸‹æ‰€æœ‰è¡¨ï¼Œæ„å»º id <-> name æ˜ å°„
                all_tables = session.query(DatasourceTable).filter(
                    DatasourceTable.ds_id == self._datasource_id
                ).all()
                if not all_tables:
                    return selected_table_names

                table_id_to_name = {table.id: table.table_name for table in all_tables}

                # å·²é€‰ä¸­çš„è¡¨å¯¹åº”çš„è¡¨ IDï¼ˆembedding / æ£€ç´¢é˜¶æ®µé€‰ä¸­çš„è¡¨ï¼‰
                selected_name_set = set(selected_table_names)
                selected_table_ids = {
                    table.id for table in all_tables if table.table_name in selected_name_set
                }
                if not selected_table_ids:
                    return selected_table_names

                selected_table_ids_str = {str(tid) for tid in selected_table_ids}

                # æ‰¾å‡ºä¸é€‰ä¸­è¡¨ç›¸å…³çš„æ‰€æœ‰å…³ç³»ï¼ˆä»»ä¸€ç«¯å‘½ä¸­å³å¯ï¼‰
                related_relations = []
                for edge in edges:
                    source = edge.get("source", {}) or {}
                    target = edge.get("target", {}) or {}
                    source_id = str(source.get("cell", "")) if source.get("cell") is not None else ""
                    target_id = str(target.get("cell", "")) if target.get("cell") is not None else ""
                    if source_id in selected_table_ids_str or target_id in selected_table_ids_str:
                        related_relations.append(edge)

                if not related_relations:
                    logger.debug(
                        f"è¡¨å…³ç³»è¡¥å……ï¼šæœªå‘ç°ä¸é€‰ä¸­è¡¨ {selected_table_names} ç›¸å…³çš„å…³ç³»è¾¹ï¼Œè·³è¿‡è¡¥å……"
                    )
                    return selected_table_names

                # æå–å…³ç³»ä¸­çš„æ‰€æœ‰è¡¨ ID
                relation_table_ids_str = set()
                for rel in related_relations:
                    source = rel.get("source", {}) or {}
                    target = rel.get("target", {}) or {}
                    source_id = str(source.get("cell", "")) if source.get("cell") is not None else ""
                    target_id = str(target.get("cell", "")) if target.get("cell") is not None else ""
                    if source_id:
                        relation_table_ids_str.add(source_id)
                    if target_id:
                        relation_table_ids_str.add(target_id)

                # æ‰¾å‡ºç¼ºå¤±çš„è¡¨ IDï¼šå…³ç³»ä¸­å‡ºç°ï¼Œä½†å½“å‰æœªé€‰ä¸­
                missing_table_ids_str = relation_table_ids_str - selected_table_ids_str

                # æ ¹æ® ID æ˜ å°„åˆ°è¡¨åï¼Œå¹¶ç¡®ä¿åœ¨ all_table_info ä¸­ï¼ˆæƒé™è¿‡æ»¤ä¹‹åï¼‰
                missing_table_names: List[str] = []
                for tid_str in missing_table_ids_str:
                    try:
                        tid = int(tid_str)
                    except (TypeError, ValueError):
                        continue
                    table_name = table_id_to_name.get(tid)
                    if table_name and table_name in all_table_info:
                        missing_table_names.append(table_name)

                if missing_table_names:
                    logger.info(
                        f"ğŸ”— è¡¨å…³ç³»è¡¥å……ï¼šä» {selected_table_names} è¡¥å…… "
                        f"{len(missing_table_names)} ä¸ªå…³è”è¡¨: {missing_table_names}"
                    )
                    extended_names = selected_table_names + [
                        name for name in missing_table_names if name not in selected_name_set
                    ]
                else:
                    extended_names = selected_table_names

                # ç”Ÿæˆ table1.field1=table2.field2 å½¢å¼çš„å¤–é”®ä¿¡æ¯ï¼Œå†™å…¥ all_table_info
                # æ„å»º node æ˜ å°„ï¼Œä¾¿äºé€šè¿‡ (cell, port) æ‰¾åˆ°å­—æ®µå
                node_by_id = {str(n.get("id")): n for n in table_nodes if n.get("id") is not None}

                def _get_field_name(cell_id: str, port_id: str) -> str:
                    """ä»å…³ç³»å›¾èŠ‚ç‚¹æˆ– DatasourceField ä¸­è§£æå­—æ®µåã€‚"""
                    # 1) ä»å‰ç«¯å…³ç³»å›¾çš„ ports ä¸­å–
                    node = node_by_id.get(cell_id)
                    if node:
                        ports = (node.get("ports") or {}).get("items") or []
                        for p in ports:
                            if str(p.get("id")) == str(port_id):
                                return (
                                    p.get("attrs", {})
                                    .get("portNameLabel", {})
                                    .get("text", "")
                                    .strip()
                                )
                    # 2) å…œåº•ï¼šä» DatasourceField.id è¯»å–
                    try:
                        if port_id and str(port_id).isdigit():
                            field = session.query(DatasourceField).filter(
                                DatasourceField.id == int(port_id)
                            ).first()
                            if field and field.field_name:
                                return field.field_name.strip()
                    except Exception:
                        pass
                    return ""

                # ä¸ºå‚ä¸å…³ç³»çš„è¡¨æ„å»º foreign_keys åˆ—è¡¨
                extracted_fks = []
                for rel in related_relations:
                    source = rel.get("source", {}) or {}
                    target = rel.get("target", {}) or {}
                    source_id = str(source.get("cell", "")) if source.get("cell") is not None else ""
                    target_id = str(target.get("cell", "")) if target.get("cell") is not None else ""
                    source_port = str(source.get("port", "")) if source.get("port") is not None else ""
                    target_port = str(target.get("port", "")) if target.get("port") is not None else ""

                    # cell id -> è¡¨å
                    try:
                        s_tid = int(source_id) if source_id and source_id.isdigit() else None
                        t_tid = int(target_id) if target_id and target_id.isdigit() else None
                    except ValueError:
                        s_tid = t_tid = None

                    s_table = table_id_to_name.get(s_tid) if s_tid is not None else None
                    t_table = table_id_to_name.get(t_tid) if t_tid is not None else None

                    if not s_table or not t_table:
                        logger.debug(f"è·³è¿‡å…³ç³»ï¼šæ— æ³•è§£æè¡¨å (source_id={source_id}, target_id={target_id})")
                        continue
                    if s_table not in all_table_info or t_table not in all_table_info:
                        logger.debug(f"è·³è¿‡å…³ç³»ï¼šè¡¨ä¸åœ¨æƒé™èŒƒå›´å†… (s_table={s_table}, t_table={t_table})")
                        continue

                    # è·å–å­—æ®µå
                    s_field = _get_field_name(source_id, source_port)
                    t_field = _get_field_name(target_id, target_port)
                    if not s_field or not t_field:
                        logger.debug(f"è·³è¿‡å…³ç³»ï¼šæ— æ³•è§£æå­—æ®µå (source_id={source_id}, port={source_port}, target_id={target_id}, port={target_port})")
                        continue

                    fk_str = f"{s_table}.{s_field}={t_table}.{t_field}"
                    extracted_fks.append(fk_str)

                    # å†™å…¥ä¸¤ç«¯è¡¨çš„ foreign_keys åˆ—è¡¨ï¼ˆé¿å…é‡å¤ï¼‰
                    for tbl in (s_table, t_table):
                        fk_list = all_table_info[tbl].setdefault("foreign_keys", [])
                        if fk_str not in fk_list:
                            fk_list.append(fk_str)

                # è®°å½•å…³ç³»æå–ç»“æœï¼ˆä»…è®°å½•æ•°é‡ï¼‰
                if extracted_fks:
                    logger.debug(f"æå–åˆ° {len(extracted_fks)} æ¡å¤–é”®å…³ç³»")
                else:
                    logger.debug("æœªæå–åˆ°å¤–é”®å…³ç³»")

                return extended_names

        except Exception as e:
            logger.warning(f"âš ï¸ è¡¨å…³ç³»è¡¥å……å¤±è´¥: {e}", exc_info=True)
            return selected_table_names

    def get_table_schema(self, state: AgentState) -> AgentState:
        """
        æ ¹æ®ç”¨æˆ·æŸ¥è¯¢ï¼Œé€šè¿‡æ··åˆæ£€ç´¢ç­›é€‰å‡ºæœ€ç›¸å…³çš„æ•°æ®åº“è¡¨ç»“æ„ã€‚
        åŒ…å«æƒé™è¿‡æ»¤ã€è¡¨å…³ç³»è¡¥å……ç­‰åŠŸèƒ½ã€‚
        """
        try:
            logger.info("ğŸ” å¼€å§‹è·å–æ•°æ®åº“è¡¨ schema ä¿¡æ¯")
            user_id = state.get("user_id")
            all_table_info = self._fetch_all_table_info(user_id=user_id)

            user_query = state.get("user_query", "").strip()
            if not user_query:
                state["db_info"] = all_table_info
                state["bm25_tokens"] = []  # æ— æŸ¥è¯¢æ—¶ï¼Œåˆ†è¯åˆ—è¡¨ä¸ºç©º
                logger.info(f"â„¹ï¸ æ— ç”¨æˆ·æŸ¥è¯¢ï¼Œè¿”å›å…¨éƒ¨ {len(all_table_info)} å¼ è¡¨")
                return state

            # è®°å½• BM25 åˆ†è¯ä¿¡æ¯ï¼Œä¾¿äºåœ¨ schema_inspector èŠ‚ç‚¹å‘ç”¨æˆ·è§£é‡Š
            try:
                bm25_tokens = self._tokenize_text(user_query)
                state["bm25_tokens"] = bm25_tokens
                if bm25_tokens:
                    logger.info(f"âœ… BM25 åˆ†è¯æˆåŠŸ: {len(bm25_tokens)} ä¸ªè¯: {bm25_tokens[:5]}")
                else:
                    logger.warning(f"âš ï¸ BM25 åˆ†è¯ç»“æœä¸ºç©ºï¼Œç”¨æˆ·æŸ¥è¯¢: {user_query}")
            except Exception as e:
                logger.error(f"âŒ BM25 åˆ†è¯è®°å½•å¤±è´¥: {e}", exc_info=True)
                state["bm25_tokens"] = []  # åˆ†è¯å¤±è´¥æ—¶ï¼Œè®¾ç½®ä¸ºç©ºåˆ—è¡¨
            
            # ç¡®ä¿ user_query ä¹Ÿåœ¨è¿”å›çš„ state ä¸­ï¼ˆè™½ç„¶å®ƒåº”è¯¥å·²ç»åœ¨åˆå§‹ state ä¸­äº†ï¼‰
            state["user_query"] = user_query

            # åˆå§‹åŒ–å‘é‡ç´¢å¼•
            self._initialize_vector_index(all_table_info)

            # æ··åˆæ£€ç´¢ - å¹¶è¡Œæ‰§è¡Œ BM25 å’Œå‘é‡æ£€ç´¢ä»¥æé«˜æ€§èƒ½
            logger.info("ğŸ” å¼€å§‹æ··åˆæ£€ç´¢ï¼šBM25 + å‘é‡æ£€ç´¢ï¼ˆå¹¶è¡Œæ‰§è¡Œï¼‰")
            
            # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæ‰§è¡Œ BM25 å’Œå‘é‡æ£€ç´¢
            with ThreadPoolExecutor(max_workers=2) as executor:
                bm25_future = executor.submit(self._retrieve_by_bm25, all_table_info, user_query)
                vector_future = executor.submit(self._retrieve_by_vector, user_query, 20)
                
                # ç­‰å¾…ä¸¤ä¸ªä»»åŠ¡å®Œæˆ
                bm25_top_indices = bm25_future.result()
                vector_top_indices = vector_future.result()
            
            logger.info(f"ğŸ“Š BM25æ£€ç´¢è¿”å› {len(bm25_top_indices)} ä¸ªç»“æœ")
            logger.info(f"ğŸ”— å‘é‡æ£€ç´¢è¿”å› {len(vector_top_indices)} ä¸ªç»“æœ")

            # è¿‡æ»¤ï¼šä»…ä¿ç•™åŒæ—¶åœ¨ BM25 å‰ 50 å’Œå‘é‡ç»“æœä¸­çš„è¡¨
            valid_bm25_set = set(bm25_top_indices[:50])
            candidate_indices = [idx for idx in vector_top_indices if idx in valid_bm25_set]
            logger.info(f"ğŸ¯ åˆæ­¥ç­›é€‰åä¿ç•™ {len(candidate_indices)} ä¸ªå€™é€‰è¡¨")

            if not candidate_indices:
                candidate_indices = bm25_top_indices[:TABLE_RETURN_COUNT]  # é™çº§
                logger.info(f"âš ï¸ å€™é€‰è¡¨ä¸ºç©ºï¼Œé™çº§ä½¿ç”¨BM25å‰{TABLE_RETURN_COUNT}ä¸ªç»“æœ")

            fused_indices = self._rrf_fusion(bm25_top_indices, candidate_indices, k=60)
            logger.info(f"ğŸ”„ RRFèåˆåå¾—åˆ° {len(fused_indices)} ä¸ªç»“æœ")

            # è¯„åˆ†ç­›é€‰
            selected_indices = []
            for idx in fused_indices:
                bm25_rank = bm25_top_indices.index(idx) + 1 if idx in bm25_top_indices else len(all_table_info) + 1
                vector_rank = (
                    vector_top_indices.index(idx) + 1 if idx in vector_top_indices else len(all_table_info) + 1
                )
                score = 1 / (60 + bm25_rank) + 1 / (60 + vector_rank)
                if score >= 0.01 and len(selected_indices) < 10:
                    selected_indices.append(idx)

            candidate_table_names = [self._table_names[i] for i in selected_indices]
            candidate_table_info = {name: all_table_info[name] for name in candidate_table_names}

            # é‡æ’åº
            reranked_results = self._rerank_with_dashscope(user_query, candidate_table_info)
            final_table_names = [name for name, _ in reranked_results][:TABLE_RETURN_COUNT]  # å– top Nï¼ˆå¯é…ç½®ï¼‰

            # å»é‡
            final_table_names = list(dict.fromkeys(final_table_names))

            # æ„å»ºè¾“å‡ºï¼ˆè¡¨å…³ç³»è¡¥å……å°†åœ¨ SQL ç”Ÿæˆé˜¶æ®µè¿›è¡Œï¼‰
            filtered_info = {name: all_table_info[name] for name in final_table_names if name in all_table_info}

            # æ‰“å°ç»“æœæ‘˜è¦
            print(f"\nğŸ” ç”¨æˆ·æŸ¥è¯¢: {user_query}")
            print("ğŸ“Š æ£€ç´¢ä¸æ’åºç»“æœ:")
            for i, table_name in enumerate(final_table_names[:TABLE_RETURN_COUNT]):
                if table_name in self._table_names:
                    bm25_idx = self._table_names.index(table_name)
                    bm25_rank = bm25_top_indices.index(bm25_idx) + 1 if bm25_idx in bm25_top_indices else "-"
                    vector_rank = vector_top_indices.index(bm25_idx) + 1 if bm25_idx in vector_top_indices else "-"
                    rerank_score = next((score for name, score in reranked_results if name == table_name), 0.0)
                    print(
                        f"  {i + 1}. {table_name:<15} | BM25: {bm25_rank:>2} | Vector: {vector_rank:>2} | Rerank: {rerank_score:.3f}"
                    )

            state["db_info"] = filtered_info
            logger.info(f"âœ… æœ€ç»ˆç­›é€‰å‡º {len(filtered_info)} ä¸ªç›¸å…³è¡¨: {list(filtered_info.keys())}")

        except Exception as e:
            logger.error(f"âŒ è·å–æ•°æ®åº“è¡¨ä¿¡æ¯å¤±è´¥: {e}", exc_info=True)
            state["db_info"] = {}
            state["execution_result"] = ExecutionResult(success=False, error="æ— æ³•è¿æ¥æ•°æ®åº“æˆ–è·å–å…ƒæ•°æ®")
            # å³ä½¿å‡ºé”™ï¼Œä¹Ÿç¡®ä¿ bm25_tokens å’Œ user_query è¢«è®¾ç½®
            if "bm25_tokens" not in state:
                state["bm25_tokens"] = []
            if "user_query" not in state:
                state["user_query"] = state.get("user_query", "")

        return state

    def execute_sql(self, state: AgentState) -> AgentState:
        """
        æ‰§è¡Œç”Ÿæˆçš„ SQL è¯­å¥ã€‚
        ä¼˜å…ˆä½¿ç”¨æƒé™è¿‡æ»¤åçš„SQLï¼ˆfiltered_sqlï¼‰ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨åŸå§‹ç”Ÿæˆçš„SQLï¼ˆgenerated_sqlï¼‰ã€‚
        """
        # ä¼˜å…ˆä½¿ç”¨æƒé™è¿‡æ»¤åçš„SQLï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨åŸå§‹ç”Ÿæˆçš„SQL
        sql_to_execute = state.get("filtered_sql") or state.get("generated_sql", "")
        sql_to_execute = sql_to_execute.strip() if sql_to_execute else ""
        
        if not sql_to_execute:
            error_msg = "SQL ä¸ºç©ºï¼Œæ— æ³•æ‰§è¡Œ"
            logger.warning(error_msg)
            state["execution_result"] = ExecutionResult(success=False, error=error_msg)
            return state

        logger.info("â–¶ï¸ æ‰§è¡Œ SQL è¯­å¥")
        # è®°å½•ä½¿ç”¨çš„SQLç±»å‹ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        if state.get("filtered_sql"):
            logger.info("ä½¿ç”¨æƒé™è¿‡æ»¤åçš„SQLæ‰§è¡Œ")
        else:
            logger.info("ä½¿ç”¨åŸå§‹ç”Ÿæˆçš„SQLæ‰§è¡Œ")
        
        try:
            with self._engine.connect() as connection:
                result = connection.execute(text(sql_to_execute))
                result_data = result.fetchall()
                columns = result.keys()
                frame = pd.DataFrame(result_data, columns=columns)
                state["execution_result"] = ExecutionResult(success=True, data=frame.to_dict(orient="records"))
                logger.info(f"âœ… SQL æ‰§è¡ŒæˆåŠŸï¼Œè¿”å› {len(result_data)} æ¡è®°å½•")
        except Exception as e:
            error_msg = f"æ‰§è¡Œ SQL å¤±è´¥: {e}"
            logger.error(error_msg, exc_info=True)
            state["execution_result"] = ExecutionResult(success=False, error=str(e))
        return state
